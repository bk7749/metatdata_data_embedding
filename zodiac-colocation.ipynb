{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "import matplotlib\n",
    "reload(matplotlib)\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt   \n",
    "import pickle as pkl\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics#v_measure_score\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_fig(fig, name, dpi=400):\n",
    "\tpp = PdfPages(name)\n",
    "\tpp.savefig(fig, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\tpp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def del_indices_list(srcList, indices):\n",
    "    copyList = deepcopy(srcList)\n",
    "    cnt = 0\n",
    "    for index in indices:\n",
    "        del copyList[index-cnt]\n",
    "        cnt+=1\n",
    "    return copyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_index_sensor_list(sensorList, srcid):\n",
    "    for index, sensor in enumerate(sensorList):\n",
    "        if sensor['source_id']==srcid:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cluster_from_srcid(clusterDict, srcid):\n",
    "    for clusterNum, clusterList in clusterDict.iteritems():\n",
    "        if srcid in clusterList:\n",
    "            return clusterNum\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_substr = lambda s, strings: all(s in x for x in strings)\n",
    "def long_substr(data):\n",
    "    substr = ''\n",
    "    if len(data) > 1 and len(data[0]) > 0:\n",
    "        for i in range(len(data[0])):\n",
    "            for j in range(len(data[0])-i+1):\n",
    "                if j > len(substr) and is_substr(data[0][i:i+j], data):\n",
    "                    substr = data[0][i:i+j]\n",
    "    return substr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import shelve dictionary containing all bacnet devices\n",
    "sensors_dict = shelve.open('metadata/bacnet_devices.shelve','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#device_list filters the NAE for a particular building. This is currently manual.\n",
    "# EBU3B\n",
    "buildingName = 'ap_m'\n",
    "naeDict = dict()\n",
    "naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "naeDict['ap_m'] = ['514', '513','604']\n",
    "naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "\n",
    "device_list = naeDict[buildingName]\n",
    "\n",
    "labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "#labeledFile = 'metadata/bonner_sensor_types_location.xlsx'\n",
    "#labeledFile = 'metadata/ap_m_with_location.csv'\n",
    "#labeledFile = 'metadata/bio_med_lib_sensor_types_location.xlsx'\n",
    "resultFile = labeledFile.replace('.xlsx', '_mislocated.xlsx')\n",
    "resultFile = labeledFile.replace('.csv', '_mislocated.xlsx')\n",
    "resultFile = resultFile.replace('metadata', 'result')\n",
    "dendoFile = labeledFile.replace('sensor_types_location.xlsx', 'dendo.pdf')\n",
    "dendoFile = labeledFile.replace('sensor_types_location.csv', 'dendo.pdf')\n",
    "dendoFile = dendoFile.replace('metadata', 'result')\n",
    "\n",
    "samplePointDict = dict()\n",
    "samplePointDict['bonner'] = [# RM-2001\n",
    "    '607_2_3000761', '607_0_3000757', '607_1_3000763', '607_2_3000758', '607_2_3000759', '607_2_3000760', \n",
    "    '607_2_3000764', '607_2_3000765', '607_2_3000766', '607_2_3000767', '607_2_3000772', '607_2_3002217', \n",
    "    '607_5_3000768', '607_5_3000769', \n",
    "    # RM-2214\n",
    "    '607_2_3000555', '607_0_3000544', '607_0_3000546', '607_0_3000559', '607_1_3000550', \n",
    "    '607_1_3000554', '607_1_3000560', '607_2_3000545', '607_2_3000551', '607_2_3000552', '607_2_3000553', \n",
    "    '607_2_3000556', '607_2_3000557', '607_2_3000558', '607_2_3000561', '607_2_3000562', \n",
    "    '607_3_3000547', '607_19_3000548', '607_19_3000549', '607_19_3000563'\n",
    "                        ]\n",
    "\n",
    "samplePointDict['ebu3b'] = [# RM-2150\n",
    "    '506_1_3000493', '506_0_3000485', '506_0_3000488', '506_0_3000489', '506_0_3000490', '506_0_3000491', \n",
    "    '506_1_3000492', '506_1_3000494', '506_1_3000495', '506_1_3000496', '506_1_3000497', '506_1_3000498', \n",
    "    '506_1_3000499', '506_1_3000500', '506_1_3015651', '506_3_3000501', '506_14_3000504', '506_1_3001889', \n",
    "    # RM-4205\n",
    "    '506_0_3001881',  '506_0_3001884', '506_0_3001885', '506_0_3001886', '506_0_3001887', '506_1_3001888', \n",
    "    '506_1_3001890', '506_1_3001891', '506_1_3001892', '506_1_3001893', '506_1_3001894', '506_1_3001895', \n",
    "    '506_1_3001896', '506_1_3015750', '506_14_3001899'\n",
    "]\n",
    "samplePointDict['ap_m'] = [ #RM-6161'\n",
    "    '604_0_3012403', '604_0_3012404', '604_0_3012413', '604_0_3012415', '604_0_3012416', '604_1_3012405',\n",
    "    '604_1_3012406', '604_1_3012407', '604_1_3012408', '604_1_3012409', '604_1_3012410', '604_1_3012411',\n",
    "    '604_1_3012412', '604_1_3012414', '604_4_3012402',\n",
    "    # RM-3108\n",
    "    '513_0_3007058', '513_0_3007060', '513_0_3007065', '513_0_3007072', '513_0_3007076', '513_0_3007077', \n",
    "    '513_1_3007063', '513_1_3007064', '513_1_3007066', '513_1_3007069', '513_1_3007070', '513_1_3007074', \n",
    "    '513_14_3007062', '513_14_3007068'\n",
    "    ]\n",
    "\n",
    "\n",
    "                           \n",
    "                           \n",
    "samplePointList = samplePointDict[buildingName]\n",
    "\n",
    "#tsLearningLabelDict = dict()\n",
    "## 1=='Temperature Setpoint'\n",
    "#tsLearningLabelDict['ebu3b'] = [  \n",
    "#                    1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, # RM-2150\n",
    "#                    1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0    # RM-4205   \n",
    "#]\n",
    "##tsLearningLabelDict['bonner'] = [\n",
    "#    1,0,0,0,0,0,0,0,0,0,\n",
    "#    0,0,0,0,1,0,0,0,0,0,\n",
    "#    0,0,0,0,0,0,0,0,0,0,\n",
    "#    0,0,0,0    # RM-4205   \n",
    "#]\n",
    "#tsLearningLabelDict['ap_m'] = [0]\n",
    "#tsLearningLabelList = tsLearningLabelDict[buildingName]\n",
    "#assert(len(tsLearningLabelList)==len(samplePointList))\n",
    "\n",
    "\n",
    "vavSampleDict = dict()\n",
    "nonvavSampleDict = dict()\n",
    "vavSampleDict['ebu3b'] = ['506_0_3000488', '506_0_3002877', '506_0_3000886']\n",
    "nonvavSampleDict['ebu3b'] = ['505_1_3000860', '505_0_3014673']\n",
    "vavSampleDict['bonner'] = ['557_1_3002743', '608_0_3000489', '607_2_3000257'] #109 17 180\n",
    "nonvavSampleDict['bonner'] = ['557_1_3003966', '557_0_3004593']\n",
    "vavSampleDict['ap_m'] = ['604_0_3013537', '513_1_3004863', '513_0_3007065']\n",
    "nonvavSampleDict['ap_m'] = ['513_1_3011367', '604_0_3013295']\n",
    "fixedVavSampleList = vavSampleDict[buildingName]\n",
    "fixedNonVavSampleList = nonvavSampleDict[buildingName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata/ap_m_sensor_types_location.csv\n"
     ]
    }
   ],
   "source": [
    "print labeledFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Labeled metadata\n",
    "\n",
    "labeledList = list()\n",
    "\n",
    "#for filename in filenameList:\n",
    "with open(labeledFile, 'rb') as fp:\n",
    "    #truthDF = pd.read_excel(fp)\n",
    "    truthDF = pd.DataFrame.from_csv(fp)\n",
    "    \n",
    "    #reader = csv.reader(fp, delimiter=',')\n",
    "    #indices = reader.next()\n",
    "    #Check if local indices are same as mainIndices\n",
    "    #for index in indices:\n",
    "        #if not index in mainIndices:\n",
    "        #    print indices\n",
    "        #    print index\n",
    "        #    print (\"indices are wrong. Please do fix the column names in the file: %s\", labeledFile)\n",
    "        #    sys.exit(\"Error message\")\n",
    "for row in truthDF.iterrows():\n",
    "    newMetadataDict = dict()\n",
    "    newMetadataDict['source_identifier'] = row[1]['Unique Identifier']\n",
    "    newMetadataDict['location'] = row[1]['Location']\n",
    "    newMetadataDict['point_type'] = row[1]['Ground Truth Point Type']\n",
    "    newMetadataDict['equip_type'] = row[1]['Equipment Type']\n",
    "    labeledList.append(newMetadataDict)\n",
    "labeledMetadata = pd.DataFrame(labeledList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse the data in the dictionary as filtered by device_list\n",
    "#Gives us a sensor_list with sensor information of a building\n",
    "\n",
    "def extract_numbers(sentence):\n",
    "    result = re.findall('(\\d+\\s?-?\\s?\\d+)|(\\d+)', sentence)\n",
    "    result = list(sum(result,()))\n",
    "    while '' in result:\n",
    "        result.remove('')\n",
    "    for i, word in enumerate(result):\n",
    "        if ' ' in word:\n",
    "            result[i] = result[i].replace(' ', '-')\n",
    "    return result\n",
    "\n",
    "\n",
    "sensor_list = []\n",
    "names_num_list = []\n",
    "names_str_list = []\n",
    "names_num_listWithDigits = [] \n",
    "sensor_type_namez=[]\n",
    "desc_list = []\n",
    "unit_list = []\n",
    "type_str_list = []\n",
    "type_list = []\n",
    "jci_names_num_list = []\n",
    "jci_names_str_list = []\n",
    "source_id_set = set([])\n",
    "for nae in device_list:\n",
    "    device = sensors_dict[nae]\n",
    "    h_dev = device['props']\n",
    "    for sensor in device['objs']:\n",
    "        h_obj = sensor['props']\n",
    "        source_id = str(h_dev['device_id']) + '_' + str(h_obj['type']) + '_' + str(h_obj['instance'])\n",
    "        \n",
    "        if h_obj['type'] not in (0,1,2,3,4,5,13,14,19):\n",
    "            continue\n",
    "        \n",
    "        if source_id in source_id_set:\n",
    "            continue\n",
    "        else:\n",
    "            source_id_set.add(source_id)\n",
    "        \n",
    "        #create individual lists\n",
    "        #remove numbers from names because they do not indicate type of sensor\n",
    "        names_num_listWithDigits.append(sensor['jci_name']) \n",
    "        sensor_type_namez.append(sensor['sensor_type'])\n",
    "        names_str_list.append(''.join([c for c in sensor['name'] if not c.isdigit()]))\n",
    "        #names_num_list.append(' '.join(re.findall('\\d+', sensor['name'])))\n",
    "        names_num_list.append(' '.join(extract_numbers(sensor['name'])))\n",
    "        \n",
    "        desc_list.append(''.join([c for c in sensor['desc'] if not c.isdigit()]))\n",
    "        #desc_list.append(' '.join(re.findall('\\d+', sensor['desc'])))\n",
    "        #jci_names_str_list.append(''.join([c for c in sensor['jci_name'] if not c.isdigit()]))\n",
    "        jci_names_str_list.append(' '.join(re.findall('[a-zA-Z]+', sensor['jci_name'])))\n",
    "        #jci_names_num_list.append(' '.join(re.findall('\\d+', sensor['jci_name'])))\n",
    "        jci_names_num_list.append(' '.join(extract_numbers(sensor['jci_name'])))\n",
    "        \n",
    "        #convert string to dictionary for categorical vectorization\n",
    "        unit_list.append({str(sensor['unit']):1})\n",
    "        type_str_list.append({str(h_obj['type_str']):1})\n",
    "        type_list.append({str(h_obj['type']):1})\n",
    "        \n",
    "        #create a flat list of dictionary to avoid using json file\n",
    "        sensor_list.append({'source_id': source_id, \n",
    "                            'name': sensor['name'], \n",
    "                            'description': sensor['desc'],\n",
    "                            'unit': sensor['unit'],\n",
    "                            'type_string': h_obj['type_str'],\n",
    "                            'type': h_obj['type'],\n",
    "                            #'device_id': h_obj['device_id'],\n",
    "                            'jci_name': sensor['jci_name'],\n",
    "                            #add data related characteristics here\n",
    "                        })\n",
    "sensor_df = pd.DataFrame(sensor_list)\n",
    "#sensor_df = sensor_df.set_index('source_id')\n",
    "#sensor_df = sensor_df.groupby(sensor_df.index).first()\n",
    "origSensorList = deepcopy(sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print names_num_list[find_index_sensor_list(sensor_list, '557_1_3003777')]\n",
    "#print jci_names_str_list[find_index_sensor_list(sensor_list, '557_1_3003777')]\n",
    "#print names_num_list[find_index_sensor_list(sensor_list, '557_1_3003776')]\n",
    "#print jci_names_str_list[find_index_sensor_list(sensor_list, '557_1_3003776')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a bag of words from sensor string metadata. Vectorize so that it can be used in ML algorithms.\n",
    "#namevect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "nameNumVect = CountVectorizer(token_pattern='\\d+')\n",
    "nameNumBow = scipy.sparse.coo_matrix(nameNumVect.fit_transform(names_num_list))\n",
    "nameStrVect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "nameStrBow = scipy.sparse.coo_matrix(nameStrVect.fit_transform(names_str_list))\n",
    "\n",
    "descVect = CountVectorizer() \n",
    "descBow = scipy.sparse.coo_matrix(descVect.fit_transform(desc_list))\n",
    "\n",
    "unitVect = DictVectorizer() \n",
    "unitBow = scipy.sparse.coo_matrix(unitVect.fit_transform(unit_list))\n",
    "\n",
    "type_str_vect = DictVectorizer() \n",
    "typeStrBow = scipy.sparse.coo_matrix(type_str_vect.fit_transform(type_str_list))\n",
    "\n",
    "typevect = DictVectorizer() \n",
    "typeBow = scipy.sparse.coo_matrix(typevect.fit_transform(type_list))\n",
    "\n",
    "jciNumVect = CountVectorizer(token_pattern='\\d+') \n",
    "jciNumBow = scipy.sparse.coo_matrix(jciNumVect.fit_transform(jci_names_num_list))\n",
    "jciStrVect = CountVectorizer() \n",
    "jciStrBow = scipy.sparse.coo_matrix(jciStrVect.fit_transform(jci_names_str_list))\n",
    "\n",
    "feature_set = jciStrVect.get_feature_names()+ \\\n",
    "              descVect.get_feature_names()+ \\\n",
    "              unitVect.get_feature_names()+ \\\n",
    "              type_str_vect.get_feature_names()+ \\\n",
    "              typevect.get_feature_names()\n",
    "\n",
    "origJciNumBow = deepcopy(jciNumBow)\n",
    "origNameNumBow = deepcopy(nameNumBow)\n",
    "origDescbow = deepcopy(descBow)\n",
    "origJciStrBow = deepcopy(jciStrBow)\n",
    "origNameStrBow = deepcopy(nameStrBow)\n",
    "origUnitBow = deepcopy(unitBow)\n",
    "origTypeBow = deepcopy(typeBow)\n",
    "origTypeStrBow = deepcopy(typeStrBow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter_bow removes words occuring too many times from dictionary\n",
    "def filter_bow(bow):\n",
    "    wordsCount = bow.sum(axis=0)\n",
    "    \n",
    "    manyWordsIndices = [i for i, x in enumerate((wordsCount>40).tolist()[0]) if x == True]\n",
    "    #print manyWordsIndices\n",
    "    properWordsIndices = [i for i in range(0,wordsCount.size) if not (i in manyWordsIndices)]\n",
    "    #print properWordsIndices\n",
    "    return scipy.sparse.csr_matrix(bow.toarray()[:,properWordsIndices])\n",
    "\n",
    "def tfidf_bow(bow):\n",
    "    transformer = TfidfTransformer()\n",
    "    return transformer.fit_transform(bow)#.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filter noise words from bow\n",
    "\n",
    "filterFunc = lambda x: tfidf_bow(x)\n",
    "#filterFunc = lambda x: filter_bow(x)\n",
    "\n",
    "#jcibow = filter_bow(origJciNumBow)\n",
    "#namebow = filter_bow(origNameNumBow)\n",
    "#descbow = filter_bow(origDescbow)\n",
    "\n",
    "\n",
    "#jciNumBow = tfidf_bow(origJciNumBow)\n",
    "#jciStrBow = tfidf_bow(origJciStrBow)\n",
    "#nameNumBow = tfidf_bow(origNameNumBow)\n",
    "#nameStrBow = tfidf_bow(origNameStrBow)\n",
    "#unitBow = tfidf_bow(origUnitBow)\n",
    "#typeBow = tfidf_bow(origTypeBow)\n",
    "#typeStrBow = tfidf_bow(origTypeStrBow)\n",
    "\n",
    "jciNumBow = filterFunc(origJciNumBow)\n",
    "jciStrBow = filterFunc(origJciStrBow)\n",
    "nameNumBow = filterFunc(origNameNumBow)\n",
    "nameStrBow = filterFunc(origNameStrBow)\n",
    "unitBow = filterFunc(origUnitBow)\n",
    "typeBow = filterFunc(origTypeBow)\n",
    "typeStrBow = filterFunc(origTypeStrBow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 126)\t0.591312045924\n",
      "  (0, 100)\t0.802623981882\n",
      "  (0, 10)\t0.078388826076\n",
      "  (1, 132)\t0.990167381579\n",
      "  (1, 10)\t0.139887656554\n",
      "  (2, 187)\t0.697547754977\n",
      "  (2, 174)\t0.703574190939\n",
      "  (2, 10)\t0.135685251118\n",
      "  (3, 174)\t0.981907310385\n",
      "  (3, 10)\t0.1893621763\n",
      "  (4, 187)\t0.835733027966\n",
      "  (4, 70)\t0.524521708133\n",
      "  (4, 10)\t0.162564706084\n",
      "  (5, 187)\t0.835733027966\n",
      "  (5, 70)\t0.524521708133\n",
      "  (5, 10)\t0.162564706084\n",
      "  (6, 206)\t0.69681760136\n",
      "  (6, 32)\t0.709554014875\n",
      "  (6, 10)\t0.104777528177\n",
      "  (7, 206)\t0.670949074284\n",
      "  (7, 33)\t0.734608054824\n",
      "  (7, 10)\t0.100887786702\n",
      "  (8, 206)\t0.65807283621\n",
      "  (8, 97)\t0.746423951239\n",
      "  (8, 10)\t0.0989516410091\n",
      "  :\t:\n",
      "  (4371, 18)\t0.694833963706\n",
      "  (4372, 170)\t0.242626458666\n",
      "  (4372, 134)\t0.737617326504\n",
      "  (4372, 39)\t0.6301214813\n",
      "  (4373, 170)\t0.185503438675\n",
      "  (4373, 124)\t0.694833963706\n",
      "  (4373, 18)\t0.694833963706\n",
      "  (4374, 170)\t0.242626458666\n",
      "  (4374, 134)\t0.737617326504\n",
      "  (4374, 39)\t0.6301214813\n",
      "  (4375, 170)\t0.185503438675\n",
      "  (4375, 124)\t0.694833963706\n",
      "  (4375, 18)\t0.694833963706\n",
      "  (4376, 170)\t0.242626458666\n",
      "  (4376, 134)\t0.737617326504\n",
      "  (4376, 39)\t0.6301214813\n",
      "  (4377, 170)\t0.185503438675\n",
      "  (4377, 124)\t0.694833963706\n",
      "  (4377, 18)\t0.694833963706\n",
      "  (4378, 170)\t0.242626458666\n",
      "  (4378, 134)\t0.737617326504\n",
      "  (4378, 39)\t0.6301214813\n",
      "  (4379, 170)\t0.185503438675\n",
      "  (4379, 124)\t0.694833963706\n",
      "  (4379, 18)\t0.694833963706\n"
     ]
    }
   ],
   "source": [
    "print type(jciStrBow)\n",
    "print jciStrBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalNumBow = scipy.sparse.hstack([\n",
    "                                 nameNumBow,\n",
    "                                 #descbow,\n",
    "                                 #unitbow,\n",
    "                                 #type_str_bow,\n",
    "                                 #typebow,\n",
    "                                 jciNumBow\n",
    "                                ]) \n",
    "bowNumArray = finalNumBow.toarray() # this is the bow for each sensor. \n",
    "origNumBowArray = deepcopy(bowNumArray)\n",
    "\n",
    "finalStrBow = scipy.sparse.hstack([\n",
    "                                 nameStrBow,\n",
    "                                 descBow,\n",
    "                                 unitBow,\n",
    "                                 typeStrBow,\n",
    "                                 typeBow,\n",
    "                                 jciStrBow\n",
    "                                ]) \n",
    "bowStrArray = finalStrBow.toarray() # this is the bow for each sensor. \n",
    "origStrBowArray = deepcopy(bowStrArray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4170\n",
      "4170\n",
      "4170\n"
     ]
    }
   ],
   "source": [
    "# List up sensors with nan location\n",
    "nanIndices = list()\n",
    "nanList = list()\n",
    "for row in labeledMetadata.iterrows():\n",
    "    location = row[1]['location']\n",
    "    if type(location)==float:\n",
    "        if math.isnan(location):\n",
    "            nanList.append(row[1]['source_identifier'])\n",
    "            continue\n",
    "    pointType = row[1]['point_type']\n",
    "    if pointType.lower()=='unknown':\n",
    "        nanList.append(row[1]['source_identifier'])\n",
    "\n",
    "# Remove sensors with zero vectors\n",
    "zerovectorIndices = np.where(np.sum(origNumBowArray, axis=1) == 0)[0]\n",
    "zerovectorList = [origSensorList[index]['source_id'] for index in zerovectorIndices]\n",
    "bowNumArray = np.delete(origNumBowArray, zerovectorIndices, axis=0)\n",
    "bowStrArray = np.delete(origStrBowArray, zerovectorIndices, axis=0)\n",
    "sensor_list = del_indices_list(origSensorList, zerovectorIndices)\n",
    "\n",
    "for index, sensor in enumerate(sensor_list):\n",
    "    if sensor['source_id'] in nanList:\n",
    "        nanIndices.append(index)\n",
    "            \n",
    "bowNumArray = np.delete(bowNumArray, nanIndices, axis=0)\n",
    "bowStrArray = np.delete(bowStrArray, nanIndices, axis=0)\n",
    "sensor_list = del_indices_list(sensor_list, nanIndices)\n",
    "#print nanList\n",
    "#print len(nanList)\n",
    "\n",
    "filteredList = zerovectorList + [nan for nan in nanList if not nan in zerovectorList]\n",
    "print len(bowNumArray)\n",
    "print len(bowStrArray)\n",
    "print len(sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'alm': 6, u'feedback': 81, u'volts': 223, u'occhtgfl': 137, u'rcdb': 163, u'onochbia': 147, u'ahtg': 3, u'dt': 60, u'current': 48, u'bchw': 14, u'zone': 230, u'easf': 64, u'occ': 134, u'clg': 35, u'byp': 25, u'hall': 91, u'lvfd': 115, u'econdif': 69, u'curr': 47, u'east': 66, u'adj': 1, u'easp': 65, u'lftr': 105, u'supflosp': 200, u'emergency': 72, u'di': 51, u'db': 49, u'band': 13, u'fan': 79, u'stpt': 195, u'dx': 61, u'press': 159, u'dmd': 55, u'rftr': 169, u'repb': 167, u'dp': 57, u'vlv': 222, u'loss': 112, u'unoccbia': 216, u'uflr': 212, u'lowosa': 114, u'dpr': 58, u'ohmin': 143, u'level': 104, u'cmd': 39, u'humidity': 99, u'rvfd': 173, u'common': 44, u'ocmin': 139, u'chwdpstpt': 30, u'zn': 229, u'differential': 53, u'pmp': 155, u'navg': 128, u'ucmnc': 211, u'setpoint': 183, u'ef': 70, u'ea': 62, u'lvlco': 117, u'flt': 86, u'ebd': 67, u'static': 192, u'flsf': 85, u'wc': 227, u'supflo': 199, u'chwdp': 29, u'cmaxflo': 38, u'tmp': 208, u'sathtg': 180, u'capacity': 27, u'esp': 77, u'nef': 130, u'speed': 189, u'overload': 151, u'rd': 166, u'smk': 186, u'unchmax': 213, u'sup': 198, u'it': 102, u'rm': 170, u'mode': 124, u'run': 172, u'wb': 226, u'power': 157, u'main': 119, u'saspstpt': 176, u'sts': 196, u'lmt': 106, u'bldg': 17, u'occclg': 135, u'water': 225, u'freq': 88, u'box': 18, u'svfd': 201, u'unocc': 215, u'last': 103, u'evfd': 78, u'chwp': 31, u'of': 141, u'chwr': 32, u'chws': 33, u'osalock': 149, u'oa': 132, u'drv': 59, u'omin': 145, u'altclgdb': 7, u'lo': 107, u'cmax': 37, u'com': 41, u'avgzn': 12, u'load': 108, u'htg': 96, u'rcpb': 165, u'co': 40, u'cm': 36, u'pos': 156, u'stndby': 194, u'ahu': 4, u'diff': 52, u'unit': 214, u'temperature': 205, u'stsp': 197, u'locally': 110, u'pb': 153, u'elect': 71, u'cooling': 45, u'low': 113, u'shed': 185, u'crac': 46, u'var': 219, u'calc': 26, u'energy': 76, u'chw': 28, u'hx': 100, u'ooclg': 148, u'north': 131, u'occhtg': 136, u'on': 146, u'lock': 111, u'torque': 209, u'dis': 54, u'econ': 68, u'pfl': 154, u'gen': 90, u'hthws': 98, u'ra': 161, u'rcit': 164, u'hi': 93, u'rst': 171, u'unt': 218, u'hold': 95, u'flah': 82, u'present': 158, u'reset': 168, u'stg': 193, u'ma': 118, u'temp': 204, u'ap': 10, u'osarst': 150, u'mtw': 125, u'flw': 87, u'dmpr': 56, u'bsmtw': 23, u'fault': 80, u'lvl': 116, u'fle': 83, u'satstpt': 181, u'vfd': 221, u'minpos': 123, u'boxmode': 19, u'bsmt': 20, u'int': 101, u'ah': 2, u'turned': 210, u'swovdif': 202, u'vp': 224, u'oclg': 138, u'rat': 162, u'aclg': 0, u'commanded': 43, u'fwdrev': 89, u'mtwsrst': 127, u'spd': 188, u'sat': 177, u'ena': 73, u'tes': 206, u'min': 122, u'chwsys': 34, u'hgt': 92, u'nd': 129, u'mig': 121, u'oat': 133, u'ocmnc': 140, u'pwr': 160, u'satclg': 178, u'mtws': 126, u'pan': 152, u'unoccupied': 217, u'max': 120, u'avg': 11, u'enble': 75, u'bsmtsf': 22, u'sys': 203, u'satecon': 179, u'hthwr': 97, u'enbl': 74, u'velocity': 220, u'eaf': 63, u'bsmtah': 21, u'lobby': 109, u'building': 24, u'dbswov': 50, u'off': 142, u'sasp': 175, u'hmax': 94, u'ampm': 8, u'ss': 190, u'alarm': 5, u'sp': 187, u'flow': 84, u'st': 191, u'bchws': 16, u'bchwr': 15, u'ohtg': 144, u'command': 42, u'time': 207, u'amps': 9, u'sa': 174, u'sf': 184, u'sdwn': 182, u'weststatic': 228}\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.21874152  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.86973083  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.25201993  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.36360195  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "[array([u'ap', u'ohtg', u'rm', u'stpt'], \n",
      "      dtype='<U12')]\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.23323047  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.85035067  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.26871316  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.38768613  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "[array([u'ap', u'ohmin', u'rm', u'stpt'], \n",
      "      dtype='<U12')]\n"
     ]
    }
   ],
   "source": [
    "print jciStrVect.vocabulary_\n",
    "print jciStrBow.toarray()[1913]\n",
    "print jciStrVect.inverse_transform(jciStrBow.toarray()[1913])\n",
    "print jciStrBow.toarray()[1912]\n",
    "print jciStrVect.inverse_transform(jciStrBow.toarray()[1912])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical agglomerative clustering \n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "\n",
    "#num_of_sensors = len(bowNumArray)\n",
    "#a = np.array(bowNumArray[:num_of_sensors])\n",
    "a = np.array(bowNumArray)\n",
    "z = linkage(a,metric='cityblock',method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "indexOffset = 2500\n",
    "dendoA = a[indexOffset:indexOffset + 500]\n",
    "dendoZ = linkage(dendoA, metric='cityblock',method='complete')\n",
    "fig = plt.figure(figsize=(45,15))\n",
    "ax = fig.add_subplot(111)\n",
    "#dendroResult = dendrogram(z, ax=ax, truncate_mode='lastp', p=3000, color_threshold='default', leaf_font_size=4)\n",
    "def llf(idx):\n",
    "    srcid = sensor_list[indexOffset + idx]['source_id']\n",
    "    return str(labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]['location'].iloc[0])\n",
    "dendroResult = dendrogram(dendoZ, ax=ax, leaf_font_size=5, leaf_label_func=llf, leaf_rotation=90)\n",
    "ax.set_ylim((-0.1,5))\n",
    "save_fig(fig, dendoFile, dpi=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:A dictionary of lists, element want to search, Output: key corresponding to the value\n",
    "def find_key_from_list_dict(d, val):\n",
    "    for key, l in d.iteritems():\n",
    "        if val in l:\n",
    "            return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#correctLabels:  4170\n",
      "#incorrectLabels:  4170\n",
      "Are they same?:  True\n"
     ]
    }
   ],
   "source": [
    "#Clustering result.\n",
    "\n",
    "b = hier.fcluster(z, 2, criterion='distance')\n",
    "\n",
    "# Make a dict of lists. Key: room number, elements in the list: \n",
    "#roomDict = defaultdict(list)\n",
    "roomDict = dict()\n",
    "correctLabels = list()\n",
    "for sensor in sensor_list:#[0:200]:\n",
    "    sourceId = sensor['source_id']\n",
    "    labeledSensor = labeledMetadata.loc[labeledMetadata['source_identifier']==sourceId]\n",
    "    #try:\n",
    "    room = labeledSensor['location'].get_values()[0]\n",
    "    correctLabels.append(room)\n",
    "    #except:\n",
    "     #   print sourceId\n",
    "    if room in roomDict.keys():\n",
    "        roomDict[room].append(sourceId)\n",
    "    else:\n",
    "        roomDict[room] = [sourceId]\n",
    "#print roomDict\n",
    "\n",
    "clusterDict = defaultdict(list)\n",
    "incorrectLabels = list()\n",
    "for clusterNum, sensor in zip(b, sensor_list):\n",
    "    clusterDict[clusterNum].append(sensor['source_id'])\n",
    "    incorrectLabels.append(clusterNum)\n",
    "print \"#correctLabels: \", len(correctLabels)\n",
    "print \"#incorrectLabels: \", len(incorrectLabels)\n",
    "print \"Are they same?: \", len(correctLabels)==len(incorrectLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 14\n",
      "2 15\n",
      "3 15\n",
      "4 14\n",
      "5 15\n",
      "6 16\n",
      "7 16\n",
      "8 15\n",
      "9 15\n",
      "10 15\n",
      "11 2\n",
      "12 18\n",
      "13 3\n",
      "14 14\n",
      "15 15\n",
      "16 15\n",
      "17 15\n",
      "18 18\n",
      "19 2\n",
      "20 15\n",
      "21 8\n",
      "22 15\n",
      "23 15\n",
      "24 60\n",
      "25 45\n",
      "26 15\n",
      "27 15\n",
      "28 15\n",
      "29 15\n",
      "30 15\n",
      "31 15\n",
      "32 15\n",
      "33 15\n",
      "34 15\n",
      "35 15\n",
      "36 15\n",
      "37 15\n",
      "38 15\n",
      "39 15\n",
      "40 15\n",
      "41 15\n",
      "42 15\n",
      "43 15\n",
      "44 3\n",
      "45 14\n",
      "46 15\n",
      "47 15\n",
      "48 3\n",
      "49 15\n",
      "50 15\n",
      "51 15\n",
      "52 14\n",
      "53 15\n",
      "54 15\n",
      "55 15\n",
      "56 14\n",
      "57 4\n",
      "58 7\n",
      "59 25\n",
      "60 20\n",
      "61 15\n",
      "62 54\n",
      "63 16\n",
      "64 30\n",
      "65 16\n",
      "66 15\n",
      "67 15\n",
      "68 15\n",
      "69 15\n",
      "70 15\n",
      "71 15\n",
      "72 15\n",
      "73 15\n",
      "74 15\n",
      "75 15\n",
      "76 15\n",
      "77 15\n",
      "78 15\n",
      "79 30\n",
      "80 43\n",
      "81 14\n",
      "82 28\n",
      "83 28\n",
      "84 28\n",
      "85 56\n",
      "86 1\n",
      "87 42\n",
      "88 14\n",
      "89 29\n",
      "90 56\n",
      "91 42\n",
      "92 14\n",
      "93 28\n",
      "94 28\n",
      "95 18\n",
      "96 14\n",
      "97 14\n",
      "98 15\n",
      "99 15\n",
      "100 16\n",
      "101 15\n",
      "102 16\n",
      "103 6\n",
      "104 11\n",
      "105 15\n",
      "106 14\n",
      "107 28\n",
      "108 28\n",
      "109 42\n",
      "110 84\n",
      "111 7\n",
      "112 29\n",
      "113 15\n",
      "114 28\n",
      "115 15\n",
      "116 14\n",
      "117 2\n",
      "118 30\n",
      "119 15\n",
      "120 16\n",
      "121 34\n",
      "122 42\n",
      "123 15\n",
      "124 22\n",
      "125 15\n",
      "126 15\n",
      "127 34\n",
      "128 15\n",
      "129 15\n",
      "130 14\n",
      "131 14\n",
      "132 14\n",
      "133 14\n",
      "134 14\n",
      "135 14\n",
      "136 15\n",
      "137 15\n",
      "138 14\n",
      "139 16\n",
      "140 29\n",
      "141 15\n",
      "142 15\n",
      "143 15\n",
      "144 5\n",
      "145 15\n",
      "146 15\n",
      "147 14\n",
      "148 15\n",
      "149 15\n",
      "150 14\n",
      "151 1\n",
      "152 14\n",
      "153 15\n",
      "154 11\n",
      "155 7\n",
      "156 9\n",
      "157 61\n",
      "158 15\n",
      "159 14\n",
      "160 15\n",
      "161 15\n",
      "162 16\n",
      "163 16\n",
      "164 11\n",
      "165 15\n",
      "166 1\n",
      "167 23\n",
      "168 14\n",
      "169 15\n",
      "170 15\n",
      "171 5\n",
      "172 25\n",
      "173 40\n",
      "174 5\n",
      "175 24\n",
      "176 15\n",
      "177 25\n",
      "178 15\n",
      "179 15\n",
      "180 15\n",
      "181 15\n",
      "182 15\n",
      "183 15\n",
      "184 15\n",
      "185 15\n",
      "186 15\n",
      "187 15\n",
      "188 15\n",
      "189 2\n",
      "190 2\n",
      "191 25\n",
      "192 2\n",
      "193 15\n",
      "194 3\n",
      "195 16\n",
      "196 14\n",
      "197 15\n",
      "198 4\n",
      "199 1\n",
      "200 15\n",
      "201 34\n",
      "202 17\n",
      "203 18\n",
      "204 31\n",
      "205 31\n",
      "206 29\n",
      "207 31\n",
      "208 15\n",
      "209 6\n",
      "210 2\n",
      "211 24\n",
      "212 16\n",
      "213 1\n",
      "214 2\n",
      "215 5\n",
      "216 3\n",
      "217 25\n",
      "218 11\n",
      "219 25\n",
      "220 15\n",
      "221 15\n",
      "222 15\n",
      "223 31\n",
      "224 31\n",
      "225 31\n",
      "226 37\n",
      "227 15\n",
      "228 6\n",
      "229 12\n",
      "230 15\n",
      "231 15\n",
      "232 15\n",
      "233 15\n",
      "234 1\n",
      "235 20\n",
      "236 32\n",
      "237 16\n",
      "238 1\n",
      "239 16\n",
      "240 1\n",
      "241 16\n",
      "242 1\n"
     ]
    }
   ],
   "source": [
    "# Measure precision\n",
    "#totalCnt = 0\n",
    "#correctCnt = 0\n",
    "#incorrectResult = dict()\n",
    "#for clusterNum, clusterList in clusterDict.iteritems():\n",
    "#    vote = defaultdict(int)\n",
    "#    totalCnt += len(clusterList)\n",
    "#    for sourceId in clusterList:\n",
    "#        foundRoom = find_key_from_list_dict(roomDict, sourceId)\n",
    "#        vote[foundRoom] += 1\n",
    "#    correctCnt += max(vote.values())\n",
    "    \n",
    "#    # list incorrectLabel\n",
    "#    currLabel = vote.keys()[vote.values().index(max(vote.values()))]\n",
    "#    for sourceId in clusterList:\n",
    "#        foundRoom = find_key_from_list_dict(roomDict, sourceId)\n",
    "#        if foundRoom.lower() != currLabel.lower():\n",
    "#            incorrectResult[sourceId] = currLabel\n",
    "    \n",
    "# Measure precision\n",
    "totalCnt = 0\n",
    "correctCnt = 0\n",
    "incorrectResult = dict()\n",
    "labelDict = defaultdict(list)\n",
    "for clusterNum, clusterList in clusterDict.iteritems():\n",
    "    vote = defaultdict(int)\n",
    "    totalCnt += len(clusterList)\n",
    "    for sourceId in clusterList:\n",
    "        foundRoom = find_key_from_list_dict(roomDict, sourceId)\n",
    "        vote[foundRoom] += 1\n",
    "    correctCnt += max(vote.values())\n",
    "    \n",
    "    # list incorrectLabel\n",
    "    currLabel = vote.keys()[vote.values().index(max(vote.values()))]\n",
    "    labelDict[currLabel].append(clusterNum)\n",
    "    for sourceId in clusterList:\n",
    "        foundRoom = find_key_from_list_dict(roomDict, sourceId)\n",
    "        if foundRoom.lower() != currLabel.lower():\n",
    "            incorrectResult[sourceId] = currLabel\n",
    "    print clusterNum, len(clusterList)\n",
    "Counter([len(clusterList) for clusterList in clusterDict.values()])\n",
    "for currLabel, clusterList in labelDict.iteritems():\n",
    "    if len(clusterList)>1:\n",
    "        lenList = [len(clusterDict[cluster]) for cluster in clusterList]\n",
    "        correctCluster = clusterList[lenList.index(max(lenList))]\n",
    "        for cluster in clusterList:\n",
    "            if correctCluster!=cluster:\n",
    "                for srcid in clusterDict[cluster]:\n",
    "                    if not srcid in incorrectResult:\n",
    "                        incorrectResult[srcid] = currLabel\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Make bows for clusters.\n",
    "\n",
    "clusterBowArray = np.zeros((len(clusterDict),len(bowStrArray[0])))\n",
    "for clusterNum, clusterList in clusterDict.iteritems():\n",
    "    \n",
    "    clusterBow = np.zeros(len(bowStrArray[0]))\n",
    "    for srcid in clusterList:\n",
    "        sensorIdx = find_index_sensor_list(sensor_list, srcid)     \n",
    "        clusterBow += bowStrArray[sensorIdx]\n",
    "        \n",
    "    clusterBowArray[clusterNum-1] = clusterBow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['604_0_3013537', '513_1_3004863', '513_0_3007065']\n",
      "['513_1_3011367', '604_0_3013295']\n",
      "Hello\n",
      "[2, 5, 96, 21, 59]\n",
      "[[ 0.          0.81096019  0.65611657 ...,  0.          0.91606636  0.        ]\n",
      " [ 0.          0.81096019  0.65611657 ...,  0.          0.91606636  0.        ]\n",
      " [ 0.          0.81096019  0.65611657 ...,  0.          0.91606636  0.        ]\n",
      " [ 0.          0.81096019  0.65611657 ...,  0.          0.91606636  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Binary classifier for VAV clusters\n",
    "random.seed()\n",
    "randomSampleFlag = False\n",
    "if randomSampleFlag:\n",
    "    vavSampleList = list()\n",
    "    nonVavSampleList = list()\n",
    "    while len(vavSampleList)+len(nonVavSampleList)<5:\n",
    "        sensorIdx = int(random.random()*len(sensor_list))\n",
    "        sensor = sensor_list[sensorIdx]\n",
    "        srcid = sensor['source_id']\n",
    "        equipType = labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]['equip_type'].iloc[0]\n",
    "        if equipType in ['vav', 'VAV'] and len(vavSampleList)<3:\n",
    "            vavSampleList.append(srcid)\n",
    "        elif not (equipType in ['vav', 'VAV']) and len(nonVavSampleList)<2:\n",
    "            nonVavSampleList.append(srcid)\n",
    "else:\n",
    "    vavSampleList = fixedVavSampleList\n",
    "    nonVavSampleList = fixedNonVavSampleList\n",
    "print vavSampleList\n",
    "print nonVavSampleList\n",
    "        \n",
    "\n",
    "indexList = list()\n",
    "vavLearningLabelList = list()\n",
    "for srcid in vavSampleList:\n",
    "    indexList.append(find_cluster_from_srcid(clusterDict, srcid))\n",
    "    vavLearningLabelList.append(1) # 1 == VAV\n",
    "for srcid in nonVavSampleList:\n",
    "    indexList.append(find_cluster_from_srcid(clusterDict, srcid))\n",
    "    vavLearningLabelList.append(0) # 0 !=VAV\n",
    "learningSampleBow = clusterBowArray[indexList]\n",
    "print \"Hello\"\n",
    "print indexList\n",
    "print learningSampleBow\n",
    "\n",
    "\n",
    "#model = RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "#model = OneClassSVM(nu=0.5, kernel=\"rbf\")#, gamma=0.1)\n",
    "#model = DPGMM(n_components = 17, covariance_type='full')\n",
    "#model = KNeighborsClassifier(n_neighbors=3)\n",
    "#model = LogisticRegression() #So far, the best\n",
    "model = GaussianNB()  # By far the best.\n",
    "#model = BernoulliNB()\n",
    "#model = AdaBoostClassifier(n_estimators=100)\n",
    "#model = MultinomialNB()\n",
    "model.fit(X=learningSampleBow, y=vavLearningLabelList)\n",
    "predProbs = model.predict_proba(clusterBowArray)\n",
    "predLabels = model.predict(clusterBowArray).tolist()\n",
    "vavDict = dict([(clusterNum, 'VAV') if vavLabel==1 else (clusterNum, '') for clusterNum, vavLabel in zip(clusterDict.keys(), predLabels)])\n",
    "vavScoreDict = dict([(clusterNum, predProb) for clusterNum, predProb in zip(clusterDict.keys(), predProbs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong, classified as nonVAV:  4\n",
      "Wrong, classified as nonVAV:  11\n",
      "Wrong, classified as nonVAV:  12\n",
      "Wrong, classified as nonVAV:  18\n",
      "Wrong, classified as nonVAV:  24\n",
      "Wrong, classified as nonVAV:  25\n",
      "Wrong, classified as nonVAV:  42\n",
      "Wrong, classified as nonVAV:  61\n",
      "Wrong, classified as nonVAV:  64\n",
      "Wrong, classified as nonVAV:  79\n",
      "Wrong, classified as nonVAV:  80\n",
      "Wrong, classified as nonVAV:  85\n",
      "Wrong, classified as nonVAV:  86\n",
      "Wrong, classified as nonVAV:  89\n",
      "Wrong, classified as nonVAV:  90\n",
      "Wrong, classified as nonVAV:  95\n",
      "Wrong, classified as nonVAV:  110\n",
      "Wrong, classified as nonVAV:  115\n",
      "Wrong, classified as nonVAV:  151\n",
      "Wrong, classified as nonVAV:  166\n",
      "Wrong, classified as nonVAV:  170\n",
      "Wrong, classified as nonVAV:  176\n",
      "Wrong, classified as nonVAV:  192\n",
      "Wrong, classified as nonVAV:  200\n",
      "Wrong, classified as nonVAV:  212\n",
      "Wrong, classified as nonVAV:  213\n",
      "Wrong, classified as nonVAV:  214\n",
      "Wrong, classified as nonVAV:  219\n",
      "Wrong, classified as nonVAV:  229\n",
      "Wrong, classified as nonVAV:  237\n",
      "Wrong, classified as nonVAV:  238\n",
      "Wrong, classified as nonVAV:  239\n",
      "Wrong, classified as nonVAV:  240\n",
      "Wrong, classified as nonVAV:  241\n",
      "Wrong, classified as nonVAV:  242\n",
      "vav precision:  1.0\n",
      "vav recall:  0.810810810811\n",
      "vav accuracy per point:  0.594964028777\n",
      "Correct VAV #:  150\n",
      "Correct Non-VAV #:  57\n",
      "Incorrect VAV #:  0\n",
      "Incorrect Non-VAV #: 35\n"
     ]
    }
   ],
   "source": [
    "# VAV classification result.\n",
    "\n",
    "correctVavCnt = 0\n",
    "correctNonVavCnt = 0\n",
    "incorrectVavCnt = 0\n",
    "incorrectNonVavCnt = 0\n",
    "vavAccuratePoints = 0\n",
    "for (clusterNum, clusterList), predLabel in zip(clusterDict.iteritems(), predLabels):\n",
    "    vote = 0\n",
    "    for srcid in clusterList:\n",
    "        equipType = labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]['equip_type'].iloc[0]\n",
    "        if equipType=='vav' or equipType=='VAV':\n",
    "            vote += 1\n",
    "        else:\n",
    "            vote -= 1\n",
    "        if (equipType in ['vav', 'VAV'] and predLabel==1) or (not equipType in ['vav', 'VAV'] and predLabel==-1):\n",
    "            vavAccuratePoints += 1\n",
    "    if vote>0 and predLabel==1:\n",
    "        correctVavCnt += 1\n",
    "    elif vote<=0 and predLabel==0:\n",
    "        correctNonVavCnt += 1\n",
    "    elif vote<=0 and predLabel == 1:\n",
    "        print \"Wrong, classified as VAV: \", clusterNum\n",
    "        incorrectVavCnt += 1\n",
    "    else:\n",
    "        print \"Wrong, classified as nonVAV: \", clusterNum\n",
    "        incorrectNonVavCnt += 1\n",
    "        \n",
    "vavPrecision = float(correctVavCnt)/float(correctVavCnt+incorrectVavCnt)\n",
    "vavRecall = float(correctVavCnt)/float(correctVavCnt+incorrectNonVavCnt)\n",
    "vavAccuracyPerPoint = float(vavAccuratePoints) / float(len(sensor_list))\n",
    "\n",
    "print \"vav precision: \", vavPrecision\n",
    "print \"vav recall: \", vavRecall\n",
    "print 'vav accuracy per point: ', vavAccuracyPerPoint\n",
    "\n",
    "\n",
    "print \"Correct VAV #: \", correctVavCnt\n",
    "print \"Correct Non-VAV #: \", correctNonVavCnt\n",
    "print \"Incorrect VAV #: \", incorrectVavCnt\n",
    "print \"Incorrect Non-VAV #:\", incorrectNonVavCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Binary Classifier for Temperature Setpoint\n",
    "\n",
    "indexList = list()\n",
    "#tsTruthLabels = list()\n",
    "tsLearningLabelList = list()\n",
    "for srcid in samplePointList:\n",
    "    index = indexList.append(find_index_sensor_list(sensor_list, srcid))\n",
    "    if labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]['point_type'].iloc[0] in ['Temperature Setpoint', 'temperature setpoint']:\n",
    "        tsLearningLabelList.append(1)\n",
    "    else:\n",
    "        tsLearningLabelList.append(0)\n",
    "        #tsTruthLabels.append(0)\n",
    "learningSampleBow = bowStrArray[indexList]\n",
    "learningSampleBow = scipy.sparse.coo_matrix(learningSampleBow)\n",
    "print tsLearningLabelList\n",
    "\n",
    "typeTruthLabels = list()\n",
    "for sensor in sensor_list:\n",
    "    srcid = sensor['source_id']\n",
    "    pointType = labeledMetadata[labeledMetadata['source_identifier']==srcid]['point_type'].iloc[0]\n",
    "    typeTruthLabels.append(pointType)\n",
    "\n",
    "#model = AdaBoostClassifier(n_estimators=100)\n",
    "#model = RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "#model = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "#model = DPGMM(n_components = 17, covariance_type='full')\n",
    "#model = KNeighborsClassifier(n_neighbors=3)\n",
    "#model = LogisticRegression()\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "#model = MultinomialNB()\n",
    "model.fit(X=learningSampleBow.toarray(), y=tsLearningLabelList)\n",
    "predProbs = model.predict_proba(bowStrArray)\n",
    "predLabels = model.predict(bowStrArray).tolist()\n",
    "thresholdProb = 0.5\n",
    "#print len(predProbs), predProbs.size\n",
    "for i, (prob, label) in enumerate(zip(predProbs, predLabels)):\n",
    "    if max(prob) < thresholdProb:\n",
    "        predLabels[i] = None\n",
    "tsPredictDict = dict([(sensor_list[i]['source_id'], predLabel) for i, predLabel in enumerate(predLabels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "4170\n",
      "4170\n",
      "0\n",
      "1\n",
      "103\n",
      "TS precision:  1.0\n",
      "TS recall:  0.990384615385\n"
     ]
    }
   ],
   "source": [
    "# Print result of the binary classifier for TS\n",
    "print np.sum(np.asarray(predLabels)==1)\n",
    "print(len(predLabels))\n",
    "print(len(typeTruthLabels))\n",
    "print np.sum(np.equal(predLabels, None))\n",
    "\n",
    "correctTsNum = np.sum([((trueLabel.lower() in ['common setpoint', 'temperature setpoint']) and (predLabel==1)) for trueLabel, predLabel in zip(typeTruthLabels, predLabels)])\n",
    "incorrectTsNum = np.sum([((trueLabel.lower() in ['common setpoint', 'temperature setpoint']) and (predLabel==0)) for trueLabel, predLabel in zip(typeTruthLabels, predLabels)])\n",
    "correctNonTsNum = np.sum([(not (trueLabel.lower() in ['common setpoint', 'temperature setpoint']) and (predLabel==0)) for trueLabel, predLabel in zip(typeTruthLabels, predLabels)])\n",
    "incorrectNonTsNum = np.sum([(not (trueLabel.lower() in ['common setpoint', 'temperature setpoint']) and (predLabel==1)) for trueLabel, predLabel in zip(typeTruthLabels, predLabels)])\n",
    "\n",
    "print incorrectTsNum\n",
    "print correctTsNum\n",
    "tsPrecision = float(correctTsNum) / float(incorrectNonTsNum + correctTsNum)\n",
    "tsRecall = float(correctTsNum) / float(incorrectTsNum + correctTsNum)\n",
    "print 'TS precision: ', tsPrecision\n",
    "print 'TS recall: ', tsRecall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2915 :  actual cooling setpoint :  3\n",
      "2916 :  actual heating setpoint :  4\n",
      "2917 :  supply air flow :  147\n",
      "2918 :  warm cool adjust :  187\n",
      "2919 :  zone temperature :  190\n",
      "3408 :  occupied cooling maximum flow :  111\n",
      "3409 :  damper command :  37\n",
      "3410 :  occupied heating maximum flow :  114\n",
      "3411 :  heating valve command :  82\n",
      "3412 :  occupied cooling setpoint :  113\n",
      "3413 :  occupied cooling minimum flow :  112\n",
      "3414 :  occupied heating minimum flow :  115\n",
      "3415 :  occupied heating setpoint :  116\n",
      "3416 :  supply air flow setpoint :  148\n",
      "4029 :  occupied command :  110\n",
      "1467 :  actual cooling setpoint :  3\n",
      "1468 :  actual heating setpoint :  4\n",
      "1469 :  damper position :  39\n",
      "1470 :  supply air flow :  147\n",
      "1471 :  warm cool adjust :  187\n",
      "1472 :  zone temperature :  190\n",
      "2475 :  cooling maximum flow :  32\n",
      "2476 :  temperature setpoint :  171\n",
      "2477 :  heating valve command :  82\n",
      "2478 :  occupied cooling minimum flow :  112\n",
      "2479 :  occupied heating minimum flow :  115\n",
      "2480 :  supply air flow setpoint :  148\n",
      "2864 :  vav box mode :  180\n",
      "2865 :  occupied command :  110\n"
     ]
    }
   ],
   "source": [
    "# Test Type Identification\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(labeledMetadata['point_type'].values)\n",
    "\n",
    "typeTestLabels = list()\n",
    "for sensor in sensor_list:\n",
    "    #print sum(labeledMetadata['source_identifier']==sensor['source_id'])\n",
    "    #print type(sensor['source_id'])\n",
    "    metadata = labeledMetadata.loc[labeledMetadata['source_identifier']==sensor['source_id']]['point_type'].iloc[0]\n",
    "    #print(type(metadata))\n",
    "    typeTestLabels.append(metadata)\n",
    "    #typeTestLabels.append(metadata['point_type']).iloc[0]\n",
    "#print typeTestLabels\n",
    "\n",
    "#srcidList = ebu3bSrcidList\n",
    "indexList = list()\n",
    "for srcid in samplePointList:\n",
    "    index = indexList.append(find_index_sensor_list(sensor_list, srcid))\n",
    "learningSampleBow = bowStrArray[indexList]\n",
    "learningSampleBow = scipy.sparse.coo_matrix(learningSampleBow)\n",
    "\n",
    "typeRawTruthList = list()\n",
    "for srcid in samplePointList:\n",
    "    typeRawTruthList.append(labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]['point_type'].iloc[0])\n",
    "typeLearningLabels = labelEncoder.transform(typeRawTruthList)\n",
    "for index, raw, label in zip(indexList, typeRawTruthList, typeLearningLabels):\n",
    "    print index, \": \", raw, \": \", label\n",
    "#model = AdaBoostClassifier(n_estimators=100)\n",
    "model = RandomForestClassifier(n_estimators=400, random_state=0)\n",
    "#model = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "#model = DPGMM(n_components = 17, covariance_type='full')\n",
    "#model = KNeighborsClassifier(n_neighbors=3)\n",
    "#model = LogisticRegression()\n",
    "#model = AdaBoostClassifier(n_estimators=100)\n",
    "#model = MultinomialNB()\n",
    "model.fit(X=learningSampleBow.toarray(), y=typeLearningLabels)\n",
    "predProbs = model.predict_proba(bowStrArray)\n",
    "predLabels = model.predict(bowStrArray).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "{'type_string': 'Analog Output', 'name': 'NAE 13 N2 Trunk 1 VAV 34 HMAX STPT', 'jci_name': 'AP&M.RM-B432C.HMAX-STPT', 'source_id': '513_1_3005325', 'type': 1, 'unit': 84, 'description': 'Occupied Heating Maximum Flow'}\n",
      "     equip_type  location                     point_type source_identifier\n",
      "1049        VAV  RM-B432C  occupied heating maximum flow     513_1_3005325\n",
      "[ 0.      0.      0.0075  0.0025  0.0025  0.02    0.0275  0.04    0.0125\n",
      "  0.0325  0.64    0.11    0.045   0.0075  0.0125  0.04    0.      0.      0.    ]\n",
      "114\n",
      "========\n",
      "{'type_string': 'Analog Output', 'name': 'NAE 13 N2 Trunk 1 VAV 34 HTG VLV', 'jci_name': 'AP&M.RM-B432C.HTG-VLV', 'source_id': '513_1_3005326', 'type': 1, 'unit': 98, 'description': 'Heating Valve Command'}\n",
      "     equip_type  location             point_type source_identifier\n",
      "1050        VAV  RM-B432C  heating valve command     513_1_3005326\n",
      "[ 0.      0.      0.      0.04    0.0075  0.8675  0.0125  0.      0.\n",
      "  0.0075  0.01    0.0075  0.005   0.015   0.0025  0.015   0.      0.005\n",
      "  0.005 ]\n",
      "82\n",
      "========\n",
      "{'type_string': 'Analog Output', 'name': 'NAE 13 N2 Trunk 2 VAV 78 OHTG STPT', 'jci_name': 'AP&M.RM-4161.OHTG-STPT', 'source_id': '513_1_3006304', 'type': 1, 'unit': 64, 'description': 'Occupied Heating Setpoint'}\n",
      "     equip_type location                 point_type source_identifier\n",
      "1380        VAV  RM-4161  occupied heating setpoint     513_1_3006304\n",
      "[ 0.0075  0.0175  0.      0.01    0.0025  0.0175  0.025   0.      0.0075\n",
      "  0.115   0.06    0.05    0.595   0.0025  0.01    0.075   0.0025  0.\n",
      "  0.0025]\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "indices = [1911, 1912, 2242]\n",
    "for idx in indices:\n",
    "    print \"========\"\n",
    "    print sensor_list[idx]\n",
    "    print labeledMetadata.loc[labeledMetadata['source_identifier']==sensor_list[idx]['source_id']]\n",
    "    print predProbs[idx]\n",
    "    print predLabels[idx]\n",
    "#filteredPredLabels = [label if prop>=0.4 else None for (label, prop) in zip(predLabels, predProbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4170\n",
      "4170\n",
      "902\n",
      "precision for labeled points:  0.999694002448\n"
     ]
    }
   ],
   "source": [
    "# Filter prediction result\n",
    "thresholdProb = 0.5\n",
    "#print len(predProbs), predProbs.size\n",
    "for i, (prob, label) in enumerate(zip(predProbs, predLabels)):\n",
    "    if max(prob) < thresholdProb:\n",
    "        predLabels[i] = None\n",
    "    \n",
    "print(len(predLabels))\n",
    "print(len(typeTruthLabels))\n",
    "print np.sum(np.equal(predLabels, None))\n",
    "decodedPredLabels = [labelEncoder.inverse_transform(label) if label else None for label in predLabels]\n",
    "correctLabelsNum = (np.sum([(testLabel==predLabel) and (testLabel!=None) for testLabel, predLabel in zip(typeTruthLabels, decodedPredLabels)]))\n",
    "print \"precision for labeled points: \", float(correctLabelsNum)/float(float(len(predLabels)-np.sum(np.equal(predLabels, None))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966243355446 0.90743244833 0.783219517667\n",
      "ap_m\n",
      "Total Number:  4380\n",
      "V Measure Score:  0.966243355446\n",
      "Coverage:  0.952054794521\n",
      "Accuracy:  0.823501199041\n",
      "nan numbers:  210\n",
      "zero vector numbers:  0\n",
      "VAV precision:  0.93085106383\n",
      "VAV recall:  0.945945945946\n",
      "VAV accuracy per point:  0.945945945946\n",
      "OrderedDict([('TS recall: ', 0.9903846153846154), ('VAV recall', 0.9459459459459459), ('VAV precision', 0.9308510638297872), ('Custom Precision', 0.8235011990407674), ('Total Number', 4380), ('VAV accuracy per point', 0.9459459459459459), ('filtered (nan-labeled || filtered) points', 210), ('V Measure score', 0.96624335544551776), ('zero vector points', 0), ('Coverage', 0.952054794520548), ('non-labeled points', 210), ('TS precision', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "# Print and export results\n",
    "\n",
    "\n",
    "def output_metadata_from_incorrect_dict(incorrectResult):\n",
    "    resultList = list()\n",
    "    for srcid, incorrectLabel in incorrectResult.iteritems():\n",
    "        labeledSensor = labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]\n",
    "        correctLabel = labeledSensor['location'].get_values()[0]\n",
    "        #print sensor_df.loc[sensor_df['source_id']==srcid]\n",
    "        row = sensor_df.loc[sensor_df['source_id']==srcid]#[1]\n",
    "        oneResult = [srcid,\n",
    "                     row['jci_name'].values[0],\n",
    "                     row['name'].values[0],\n",
    "                     incorrectLabel,\n",
    "                     correctLabel]     \n",
    "        resultList.append(oneResult)\n",
    "    try:\n",
    "        resultDF = pd.DataFrame(resultList, columns=['source_identifier', 'jci_name', 'name', 'incorrect clustering', 'correct location'])\n",
    "    except:\n",
    "        resultDF = pd.DataFrame()\n",
    "    return resultDF\n",
    "\n",
    "def output_metadata_from_srcids(srcidList):\n",
    "    resultList = list()\n",
    "    for srcid in srcidList:\n",
    "        labeledSensor = labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]\n",
    "        correctLabel = labeledSensor['location'].get_values()[0]\n",
    "        row = sensor_df.loc[sensor_df['source_id']==srcid]#[1]\n",
    "        oneResult = [srcid,\n",
    "                     row['jci_name'].values[0],\n",
    "                     row['name'].values[0],\n",
    "                     None,\n",
    "                     correctLabel]     \n",
    "        resultList.append(oneResult)\n",
    "    try:\n",
    "        resultDF = pd.DataFrame(resultList, columns=['source_identifier', 'jci_name', 'name', 'incorrect clustering', 'correct location'])\n",
    "    except:\n",
    "        resultDF = pd.DataFrame()\n",
    "    return resultDF\n",
    "\n",
    "def output_metadata_from_cluster_dict(clusterDict=None, vavDict=None, vavScoreDict=None, tsDict=None):\n",
    "    resultList = list()\n",
    "    for clusterNum, clusterList in clusterDict.iteritems():\n",
    "        for srcid in clusterList:\n",
    "            labeledSensor = labeledMetadata.loc[labeledMetadata['source_identifier']==srcid]\n",
    "            correctLabel = labeledSensor['location'].get_values()[0]\n",
    "            if vavDict!=None:\n",
    "                correctEquipLabel = labeledSensor['equip_type'].get_values()[0]\n",
    "                vavPredict = vavDict[clusterNum]\n",
    "            else:\n",
    "                correctEquipLabel = ''\n",
    "                vavPredict = ''\n",
    "            if tsDict!=None:\n",
    "                correctTypeLabel = labeledSensor['point_type'].iloc[0]\n",
    "                tsPredict = tsDict[srcid]\n",
    "            else:\n",
    "                correctTypeLabel = ''\n",
    "                tsPredict = ''\n",
    "            idx = find_index_sensor_list(sensor_list, srcid)\n",
    "            bow = bowNumArray[idx]\n",
    "            bowStr = str(bow[bow>0])\n",
    "            bowSum = np.sum(bow)\n",
    "            if vavScoreDict != None:\n",
    "                vavScore = str(vavScoreDict[clusterNum])\n",
    "            else:\n",
    "                vavScore = ''\n",
    "            \n",
    "            \n",
    "            row = sensor_df.loc[sensor_df['source_id']==srcid]#[1]\n",
    "            oneResult = [srcid,\n",
    "                         row['jci_name'].values[0],\n",
    "                         row['name'].values[0],\n",
    "                         clusterNum,\n",
    "                         correctLabel,\n",
    "                         vavPredict,\n",
    "                         correctEquipLabel,\n",
    "                         vavScore,\n",
    "                         bowStr,\n",
    "                         bowSum,\n",
    "                         tsPredict,\n",
    "                         correctTypeLabel\n",
    "                        ]     \n",
    "            resultList.append(oneResult)\n",
    "    resultDF = pd.DataFrame(resultList, columns=['source_identifier', 'jci_name', 'name', 'cluster number', 'correct location', 'vav predict', 'correct equip type', 'vav pred score', \n",
    "                                                 'bow string', 'sum of bow', 'TS preddict', 'true point type'])\n",
    "    return resultDF\n",
    "\n",
    "\n",
    "precision = float(totalCnt-len(incorrectResult))/float(totalCnt)\n",
    "vScore = metrics.v_measure_score(correctLabels, incorrectLabels)\n",
    "amiScore = metrics.adjusted_mutual_info_score(correctLabels, incorrectLabels)\n",
    "arScore = metrics.adjusted_rand_score(correctLabels, incorrectLabels)\n",
    "print vScore, amiScore, arScore\n",
    "totalNum = len(origSensorList)\n",
    "nanNum = len(nanList)\n",
    "zerovectorNum = len(zerovectorList)\n",
    "filteredNum = len(filteredList)\n",
    "coverage = float(totalCnt)/float(len(origSensorList))\n",
    "vavPrecision = float(correctVavCnt)/float(correctVavCnt+incorrectVavCnt)\n",
    "vavRecall = float(correctVavCnt)/float(correctVavCnt+incorrectNonVavCnt)\n",
    "vavAccuracyPerPoint = float(vavAccuratePoints) / len(sensor_list)\n",
    "print buildingName\n",
    "print \"Total Number: \", totalNum\n",
    "print \"V Measure Score: \", vScore\n",
    "print \"Coverage: \", coverage\n",
    "print \"Accuracy: \", precision\n",
    "print \"nan numbers: \", nanNum\n",
    "print \"zero vector numbers: \", zerovectorNum\n",
    "#print \"filtered numbers: \", filteredList\n",
    "print \"VAV precision: \", vavPrecision\n",
    "print \"VAV recall: \", vavRecall\n",
    "print \"VAV accuracy per point: \", vavRecall\n",
    "\n",
    "summaryDict = {'Total Number' : totalNum,\n",
    "                 \"V Measure score\": vScore,\n",
    "                 \"Custom Precision\" : precision,\n",
    "                 \"Coverage\" : coverage,\n",
    "                 \"non-labeled points\" : nanNum,\n",
    "                 \"zero vector points\" : zerovectorNum,\n",
    "                 \"filtered (nan-labeled || filtered) points\" : filteredNum,\n",
    "                \"VAV precision\": vavPrecision,\n",
    "                \"VAV recall\": vavRecall,\n",
    "               \"VAV accuracy per point\": vavRecall,\n",
    "               \"TS precision\": tsPrecision,\n",
    "               'TS recall: ': tsRecall\n",
    "                }\n",
    "summaryDict = OrderedDict(summaryDict)\n",
    "print summaryDict\n",
    "summaryDF = pd.DataFrame(data=[summaryDict.values()], columns=summaryDict.keys())\n",
    "incorrectResultDF = output_metadata_from_incorrect_dict(incorrectResult)\n",
    "zerovectorResultDF = output_metadata_from_srcids(zerovectorList)\n",
    "nonlabeledResultDF = output_metadata_from_srcids(nanList)\n",
    "clusterResultDF = output_metadata_from_cluster_dict(clusterDict=clusterDict, vavDict=vavDict, vavScoreDict=vavScoreDict, tsDict = tsPredictDict)\n",
    "writer = pd.ExcelWriter(resultFile)\n",
    "summaryDF.to_excel(writer, 'summary', index=False)\n",
    "incorrectResultDF.to_excel(writer, 'Incorrect Labels', index=False)\n",
    "nonlabeledResultDF.to_excel(writer, 'Nonlabeled Labels', index=False)\n",
    "zerovectorResultDF.to_excel(writer, 'Zero-vector Labels', index=False)\n",
    "clusterResultDF.to_excel(writer, 'Clusters', index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
